{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlmQIFSLZDdc"
   },
   "source": [
    "# Confirm TensorFlow can see the GPU\n",
    "\n",
    "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "3IEVK-KFxi5Z",
    "outputId": "bb971f50-05f3-4840-9ffd-d7aaf9ddc59a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "print('Device name: {}'.format(device_name))\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2L10VMH5oq1s"
   },
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12767
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "7YNnatlNbWce",
    "outputId": "5303ca3b-d335-432d-e700-d168473824cf"
   },
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!sudo add-apt-repository ppa:djcj/hybrid -y\n",
    "!sudo apt-get update -y\n",
    "!sudo apt-get install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "B7ZZrgKm1hri"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.output\n",
    "import IPython.display as ipd\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "PATH = '../datasets' #'/root/datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUx6qBMGo2Ci"
   },
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 23931
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "xCPdrVkSoYA5",
    "outputId": "398af21c-528c-49c1-a4bb-5915215bbbe1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf ./sample_data/*\n",
    "!rm -rf /root/datasets\n",
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_download_path = os.path.expanduser(os.path.join(PATH, 'raw'))\n",
    "try:\n",
    "    os.makedirs(local_download_path)\n",
    "except: pass\n",
    "\n",
    "# 2. Auto-iterate using the query syntax\n",
    "#    https://developers.google.com/drive/v2/web/search-parameters\n",
    "file_list = drive.ListFile({'q': \"'1hNxLdzo8GyzwD6EGuJNdye6N1h0mcRlX' in parents\"}).GetList()\n",
    "\n",
    "with tqdm(total=len(file_list), file=sys.stdout) as pbar:\n",
    "    for f in file_list:\n",
    "        fname = os.path.join(local_download_path, f['title'])\n",
    "        pbar.write('[title: %s, id: %s] downloading to %s' % (f['title'], f['id'], fname))\n",
    "        pbar.update(1)\n",
    "        f_ = drive.CreateFile({'id': f['id']})\n",
    "        f_.GetContentFile(fname)\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_download_path = os.path.expanduser(os.path.join(PATH, 'noise'))\n",
    "try:\n",
    "    os.makedirs(local_download_path)\n",
    "except: pass\n",
    "\n",
    "# 2. Auto-iterate using the query syntax\n",
    "#    https://developers.google.com/drive/v2/web/search-parameters\n",
    "file_list = drive.ListFile({'q': \"'18WwnYEglonXHw7cHZnL_iYTrNAqaqliL' in parents\"}).GetList()\n",
    "\n",
    "with tqdm(total=len(file_list), file=sys.stdout) as pbar:\n",
    "    for f in file_list:\n",
    "        fname = os.path.join(local_download_path, f['title'])\n",
    "        pbar.write('[title: %s, id: %s] downloading to %s' % (f['title'], f['id'], fname))\n",
    "        pbar.update(1)\n",
    "        f_ = drive.CreateFile({'id': f['id']})\n",
    "        f_.GetContentFile(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 21830
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "-zwrtjNgoL63",
    "outputId": "2bc18409-2f63-41c7-ee02-098d152525de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = '../datasets'\n",
    "path = os.path.join(PATH, 'input')\n",
    "counter = 0\n",
    "for parent, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        ext = os.path.splitext(file)[1][1:]\n",
    "        if not parent.endswith(ext):\n",
    "            dest = os.path.join(parent, ext)\n",
    "            if not os.path.exists(dest):\n",
    "                os.mkdir(dest)\n",
    "            src, dest = os.path.join(parent, file), os.path.join(dest, file)\n",
    "            os.rename(src, dest)\n",
    "            counter += 1\n",
    "            print('Moved {} to {}'.format(src, dest))\n",
    "print('Moved {} files.'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GhfAfo1GpFDY"
   },
   "outputs": [],
   "source": [
    "def stretch(data, r = 1.0):\n",
    "    return librosa.effects.time_stretch(data, r)\n",
    "    \n",
    "def noise(x, **params):\n",
    "    noise = np.zeros(x.shape)\n",
    "    noise_type = params['type'] if 'type' in params else 'white'\n",
    "    if noise_type == 'white':\n",
    "        distribution = params['distribution'] if 'distribution' in params else 'gaussian'\n",
    "        sigma = params['sigma'] if 'sigma' in params else 0.005\n",
    "        if distribution == 'gaussian':\n",
    "            noise = np.random.normal(scale = sigma, size = x.shape)\n",
    "        elif distribution == 'uniform':\n",
    "            noise = np.random.uniform(low = -sigma, high = sigma, size = x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def plot(x, sr, title = 'Raw Data'):\n",
    "    plt.figure(figsize = (14, 5))\n",
    "    plt.title(title)\n",
    "    librosa.display.waveplot(x, sr = sr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 21847
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "uHN8iiZt2FcV",
    "outputId": "67f6bcc5-f1d2-4d5a-c31c-043fae9d5123",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = '../datasets'\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame(columns = ['input', 'output'])\n",
    "path = os.path.join(PATH, 'output')\n",
    "noisy_path = os.path.join(PATH, 'input')\n",
    "if not os.path.exists(noisy_path):\n",
    "    os.mkdir(noisy_path)\n",
    "counter = 0\n",
    "SIGMA_LOW = 1e-3\n",
    "SIGMA_HIGH = 5e-2\n",
    "NOISE_PARAMS = {'type': 'white', 'distribution': 'gaussian', 'sigma': 0.002}\n",
    "for parent, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        src = os.path.join(parent, file)\n",
    "        x, sr = librosa.load(src)\n",
    "        NOISE_PARAMS['sigma'] = np.random.uniform(SIGMA_LOW, SIGMA_HIGH)\n",
    "        x = noise(x, **NOISE_PARAMS)\n",
    "        dest = os.path.join(noisy_path, '{}-{}'.format('-'.join([str(v) for v in NOISE_PARAMS.values()]), file))\n",
    "        try:\n",
    "            librosa.output.write_wav(dest, x, sr)\n",
    "        except:\n",
    "            print('Leaving...')\n",
    "            break\n",
    "        df.loc[counter] = [dest, src]\n",
    "        counter += 1\n",
    "        print('Generated new noisy audio file and saved in {}!'.format(dest))\n",
    "print('Generated {} new noisy audio files.'.format(counter))\n",
    "df.to_csv(os.path.join(PATH, 'raw2noisy.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Zd1syA90ShiI",
    "outputId": "fe7921c6-dc08-4525-e69a-1a37dc828de2"
   },
   "outputs": [],
   "source": [
    "df.loc[0, 'input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "bzGUVZvmT1lr",
    "outputId": "3d1fa62d-21b6-49d4-9ef2-c225f6c88139"
   },
   "outputs": [],
   "source": [
    "input_file = df.loc[0, 'input']\n",
    "x, sr = load(input_file)\n",
    "ipd.Audio(x, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "25uaWjmzUqCi",
    "outputId": "cefffb04-09e6-41c8-f5c8-6bcbbadcec04"
   },
   "outputs": [],
   "source": [
    "output_file = df.loc[0, 'output']\n",
    "y, sr = load(output_file)\n",
    "ipd.Audio(y, rate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrianohrl/anaconda3/lib/python3.5/site-packages/numba/errors.py:104: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.output\n",
    "import IPython.display as ipd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "PATH = '../datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "5HCtacad96XW",
    "outputId": "f5a437c7-c777-4f7f-f32d-929d36e41edc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "345it [05:45,  1.17s/it]\n",
      "148it [02:26,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "N = 32000\n",
    "TEST_SIZE = 0.3\n",
    "SEED = 42\n",
    "df = pd.read_csv(os.path.join(PATH, 'raw2noisy.csv'))\n",
    "M = len(df) // 2\n",
    "df_train, df_test = train_test_split(df.head(M), test_size = TEST_SIZE, random_state = SEED)\n",
    "df_train, df_test = df_train.reset_index(), df_test.reset_index()\n",
    "M_train, M_test = len(df_train), len(df_test)\n",
    "X_train, Y_train = np.zeros((M_train, N, 1)), np.zeros((M_train, N, 1))\n",
    "for i, row in tqdm(df_train.head(M).iterrows()):\n",
    "    try:\n",
    "        X_train[i, :, 0], Y_train[i, :, 0] = librosa.load(row['input'])[0][:N], librosa.load(row['output'])[0][:N]\n",
    "    except:\n",
    "        continue\n",
    "X_test, Y_test = np.zeros((M_test, N, 1)), np.zeros((M_test, N, 1))\n",
    "for i, row in tqdm(df_test.head(M).iterrows()):\n",
    "    try:\n",
    "        X_test[i, :, 0], Y_test[i, :, 0] = librosa.load(row['input'])[0][:N], librosa.load(row['output'])[0][:N]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbLxM94A2Wm5"
   },
   "source": [
    "# Encoder-Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "W6_z1_uy19II",
    "outputId": "c24a7779-ed1c-4d42-bae7-2b237ee57d10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, RepeatVector, TimeDistributed, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "ZPKqMNCQ2enh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (LSTM)               (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 32000, 128)        0         \n",
      "_________________________________________________________________\n",
      "decoder (LSTM)               (None, 32000, 128)        131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 32000, 1)          129       \n",
      "=================================================================\n",
      "Total params: 198,273\n",
      "Trainable params: 198,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N_CELLS = 128\n",
    "DROPOUT = 0.25\n",
    "LEARNING_RATE = 0.001\n",
    "OPTIMIZER = 'adam'\n",
    "LOSS = 'mse'\n",
    "ACTIVATION = 'tanh'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(N_CELLS, activation = ACTIVATION, input_shape = (N, 1), name = 'encoder'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(RepeatVector(N))\n",
    "model.add(LSTM(N_CELLS, activation = ACTIVATION, return_sequences = True, name = 'decoder'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(optimizer = Adam(lr = LEARNING_RATE), loss = LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1754
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ObWUtpphDF2V",
    "outputId": "5665b2c3-b039-45c7-91f9-a7d18b34989a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 345 samples, validate on 148 samples\n",
      "Epoch 1/20\n",
      "330/345 [===========================>..] - ETA: 1:38 - loss: 0.0191"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "PATIENCE = 3\n",
    "MIN_DELTA = 1e-2\n",
    "# callbacks = [\n",
    "#     EarlyStopping(\n",
    "#         monitor='val_loss', \n",
    "#         min_delta = MIN_DELTA, \n",
    "#         patience = PATIENCE, \n",
    "#         verbose = 0, \n",
    "#         mode = 'auto', \n",
    "#         restore_best_weights = True\n",
    "#     )\n",
    "# ]\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    validation_data = (X_test, Y_test),\n",
    "#     callbacks = callbacks,\n",
    "    epochs = EPOCHS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = time()\n",
    "df_report_filename = os.path.join(PATH, 'report.csv')\n",
    "if not os.path.exists(report_df_filename):\n",
    "    columns = [\n",
    "        'timestamp', \n",
    "        'json', 'h5', \n",
    "        'n', \n",
    "        'm', \n",
    "        'test_size', \n",
    "        'seed', \n",
    "        'n_cells', \n",
    "        'dropout', \n",
    "        'optimizer', \n",
    "        'learning_rate', \n",
    "        'loss', \n",
    "        'activation',\n",
    "        'epochs',\n",
    "        'batch_size',\n",
    "        'patience',\n",
    "        'min_delta'\n",
    "    ]\n",
    "    df_report = pd.DataFrame()\n",
    "else:\n",
    "    df_report = pd.read_csv(report_df_filename)\n",
    "json = model.to_json()\n",
    "json_filename = os.path.join(PATH, 'model-{}.json'.format(int(ts)))\n",
    "h5_filename = os.path.join(PATH, 'model-{}.h5'.format(int(ts)))\n",
    "with open(json_filename, 'w') as file:\n",
    "    file.write(json)\n",
    "    file.close()\n",
    "    model.save_weights(h5_filename)\n",
    "    df_report.loc[len(df_report)] = [\n",
    "        str(ts), \n",
    "        N, \n",
    "        M, \n",
    "        TEST_SIZE, \n",
    "        SEED, \n",
    "        N_CELLS, \n",
    "        DROPOUT, \n",
    "        OPTIMIZER, \n",
    "        LEARNING_RATE, \n",
    "        LOSS, \n",
    "        ACTIVATION,\n",
    "        EPOCHS,\n",
    "        BATCH_SIZE,\n",
    "        PATIENCE,\n",
    "        MIN_DELTA\n",
    "    ]\n",
    "    print('Saved model to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(json_filename, 'r') as file:\n",
    "    json = file.read()\n",
    "    file.close()\n",
    "    model = model_from_json(json)\n",
    "    model.load_weights(h5_filename)\n",
    "    print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "VCdxsvH-eg1v",
    "outputId": "30699ca7-5faa-431d-c5ca-e13de3e578d4"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X, verbose = 0)\n",
    "save(os.path.join(path, 'pred.wav'), Y_pred.flatten(), sr)\n",
    "ipd.Audio(Y_pred.flatten(), rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "AHhE49SfbAac",
    "outputId": "601e4ac7-932c-416d-c0b4-ed5227d1d5a5"
   },
   "outputs": [],
   "source": [
    "#train_mse = history.history['mse']\n",
    "#validation_mse = history.history['val_mean_squared_error']\n",
    "train_loss = history.history['loss']\n",
    "#validation_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "#plt.plot(epochs, train_mse, 'b-', label = 'Training MSE')\n",
    "#plt.plot(epochs, validation_mse, 'r-', label = 'Validation MSE')\n",
    "#plt.title('Training and validation MSE')\n",
    "#plt.legend()\n",
    "\n",
    "fig = plt.figure(figsize = (14, 5))\n",
    "plt.plot(epochs, train_loss, 'b-', label = 'Training loss')\n",
    "#plt.plot(epochs, validation_loss, 'r-', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig.savefig('../annotations/images/train_{}.eps'.format(int(ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "kVrMxE5mjVKB",
    "outputId": "7ac034c0-0224-4831-aa78-1e505fa73976"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14, 5))\n",
    "librosa.display.waveplot(Y_pred.flatten(), sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "hxDzzWGqjx78"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_autoencoder",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
